\documentclass{beamer}
\usepackage[UTF8]{ctex}
\usepackage{graphicx} 
\usepackage{float} 
\usepackage{subfigure}
\usepackage{algorithm,algorithmic}
\usepackage{tikzit}

\input{figs/sample.tikzstyles}
\usetheme{Madrid}
\usecolortheme{default}

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title %optional
{基于同构测试的图神经网络简介}

\author % (optional)
{How Powerful are Graph Neural Networks?}

\date % (optional)
{\today}

%End of title page configuration block
%------------------------------------------------------------



%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------


\begin{document}

%The next statement creates the title page.
\frame{\titlepage}


%---------------------------------------------------------
%This block of code is for the table of contents after
%the title page
\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}
%---------------------------------------------------------


\section{图同构测试}

%---------------------------------------------------------
%Changing visivility of the text
\begin{frame}

  \frametitle{图同构}

  \begin{block}{定义}
    对于两个无向图 $G$ 和 $H$，若存在它们顶点之间的双射 $f:V(G) \rightarrow V(H)$，
    使得 $G$ 中的顶点 $u$ 和 $v$ 相邻当且仅当 $H$ 中的顶点 $f(u)$ 和 $f(v)$ 相邻，
    则称 $G$ 和 $H$ 同构。
  \end{block}
  \centering
  \includegraphics[scale=0.1]{figs/Graph_isomorphism_a.png}
  \includegraphics[scale=0.2]{figs/Graph_isomorphism_b.png}

\end{frame}

%---------------------------------------------------------


%---------------------------------------------------------
%Example of the \pause command
\begin{frame}
  \frametitle{图同构的必要不充分条件}
  \begin{center}
    \scalebox{0.8}{
      \begin{minipage}{1\linewidth}
        \begin{algorithm}[H]
        \begin{algorithmic}[1]
        \IF{$V(G) \ne V(H)$}
          \RETURN \FALSE
        \ENDIF
        \FOR{$v$ in $V(G)$ and $V(H)$}
          \STATE $L_v^{(0)} = $ the same label
        \ENDFOR
        \FOR{$i=1$ to $|V(G)|$}
          \FOR{$v$ in $V(G)$ and $V(H)$}
            \STATE $L_v^{(i)} = hash (\{ L_u^{(i-1)} : u \in \mathcal{N}(v) \})$
          \ENDFOR
          \IF{$\{L^{(i)}_v : v \in V(G)\} \ne \{L^{(i)}_v : v \in V(H)\}$}
            \RETURN \FALSE
          \ENDIF
          \IF{no change in the partition between $L^{(i)}$ and $L^{(i-1)}$}
            \STATE \textbf{break}
          \ENDIF
        \ENDFOR
        \RETURN \TRUE
        \end{algorithmic}
        \caption{Weisfeiler-Lehman Test}
        \label{alg:seq}
        \end{algorithm}
      \end{minipage}
      }
  \end{center}
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{Weisfeiler-Lehman Test 示例}
  \ctikzfig{figs/WL-1}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{Weisfeiler-Lehman Test 示例}
  \ctikzfig{figs/WL-2}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{Weisfeiler-Lehman Test 示例}
  \ctikzfig{figs/WL-3}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{Weisfeiler-Lehman Test 示例}
  \ctikzfig{figs/WL-4}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{Weisfeiler-Lehman Test 示例}
  \ctikzfig{figs/WL-8}
  标签分布与上一轮相同，说明迭代收敛。
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{Weisfeiler-Lehman Test}
  \begin{block}{WL-Test 解释}
    达到稳定时，图的标签集合是聚合运算的不动点，说明有相同标签的结点
    与其他结点有相似的邻接关系。但标签集合无法决定唯一的邻接关系。
  \end{block}
  \begin{alertblock}{WL-Test 是图同构的必要不充分条件}
    两个同构图经过上述变换后得到的标签集合一定相同，但得到相同的标签集合只能说明两个图\textbf{可能}同构。
  \end{alertblock}
  \ctikzfig{figs/WL-ce}

\end{frame}
%---------------------------------------------------------

\section{图神经网络结构}

%---------------------------------------------------------
\begin{frame}

  \frametitle{基于图数据的任务举例}
  图数据任务隐含了如下假设：结点或图的标签除了与收集的特征有关，还与图的邻接关系有关，故可以通过
  扩展 WL-Test 来设计神经网络。
  \begin{block}{结点分类}
    每个结点 $v \in V$ 都有特征向量 $x_v$ 及对应的标签 $y_v$，
    要求学习每个结点的嵌入向量 $h_v$ 和一种映射 $f$，使得结点的标签可以通过 $\hat{y}_v = f(h_v)$ 预测。
  \end{block}
  \begin{block}{图分类}
    有一系列图 $\{G_1, ..., G_N\}$ 及其标签 $\{y_1, ..., y_N\}$。
    要求学习每个图的嵌入向量 $h_G$ 和一种映射 $g$，使得图的标签可以通过 $\hat{y}_G = g(h_G)$ 预测。
  \end{block}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图神经网络的计算过程}
    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \FOR{$v$ in $V(G)$}
        \STATE $h_v^{(0)} = x_v$
      \ENDFOR
      \FOR{$k=1$ to $K$}
        \STATE $a_v^{(k)} = AGGREGATE^{(k)}(\{h_u^{(k-1)} : u \in \mathcal{N}(v)\})$
        \STATE $h_v^{(k)} = COMBINE^{(k)}(h_v^{(k-1)}, a_v^{(k)})$
      \ENDFOR
      \STATE $h_G = READOUT(\{h_v^{(K)} : v \in G\})$
    \end{algorithmic}
    \caption{GNN}
    \label{alg:gnn}
    \end{algorithm}
      
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图神经网络的计算过程}
      
  $AGGREGATE$：聚集邻居结点的信息，如 
  $$a_v^{(k)} = MAX(\{RELU(W_a \cdot h_u^{(k-1)}) : u \in \mathcal{N}(v)\}).$$
  $COMBINE$：引入中心结点的信息，如
  $$h_v^{(k)} = W_c \cdot CONCAT(h_v^{(k-1)}, a_v^{(k)}).$$
  在卷积图神经网络中，上述两个过程合为一个：
  $$h_v^{(k)} = RELU(W \cdot MEAN \{h_u^{(k-1)} : u \in \mathcal{N}(v) \cup \{v\} \}).$$
  $READOUT$：聚集结点的嵌入从而生成图的嵌入，结果与输入结点的顺序无关，如按位取最大值。
\end{frame}
%---------------------------------------------------------

\section{图神经网络的表达能力}

%---------------------------------------------------------
\begin{frame}

  \frametitle{图神经网络与 WL-Test 的比较}
  \begin{alertblock}{定理：图神经网络的表达能力不强于 WL-Test}
    设 $G_1$ 和 $G_2$ 为两个不同构的图，如果一个图神经网络 $\mathcal{A}: \mathcal{G} \rightarrow \mathbb{R}^d$
    将 $G_1$ 和 $G_2$ 映射到不同的嵌入，则 WL-Test 会认为 $G_1$ 和 $G_2$ 不同构。
  \end{alertblock}
  \begin{block}{证明：图神经网络的表达能力不强于 WL-Test}
    以特征向量作为结点的标签，若 WL-Test 认为 $G_1$ 和 $G_2$ 同构，则每一轮迭代 $G_1$ 和 $G_2$ 都有相同的标签集合。
    
    由归纳法可证：对于任意两个结点 $u$ 和 $v$，若 WL-Test 中的标签 $L_u^{(i)} = L_v^{(i)}$，则 GNN 中的嵌入向量
    $h_u^{(i)} = h_v^{(i)}$。

    在 WL-Test 的最后一轮迭代时，$G_1$ 和 $G_2$ 具有相同的标签集合，从而说明 GNN 的结点嵌入集合是相同的，故
    GNN 的图嵌入是相同的。
  \end{block}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图神经网络与 WL-Test 的比较}
  \begin{alertblock}{定理：单射图神经网络的表达能力与 WL-Test 等价}
    设 $\mathcal{A}: \mathcal{G} \rightarrow \mathbb{R}^d$ 是一个有充分多层的图神经网络，若满足如下条件：

    (1) 更新嵌入向量的过程是单射，即
    $$ h_v^{(k)} = \phi(h_v^{(k-1)}, f(\{h_u^{(k-1)} : u \in \mathcal{N}(v)\})) $$
    中的 $\phi$ 相对于两个向量输入是单射，$f$ 相对于邻居向量集合是单射。

    (2) 生成图嵌入向量的 $READOUT$ 函数相对于所有结点的向量集合 $\{h_v^{(k)} : v \in V(G)\}$ 是单射。

    则该图神经网络会把任意两个 WL-Test 判断为不同构的图映射到两个不同的嵌入向量。

  \end{alertblock}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图神经网络与 WL-Test 的比较}
  \begin{block}{证明：单射图神经网络的表达能力与 WL-Test 等价}
    设 $G_1$ 和 $G_2$ 经过 $K$ 轮迭代后被 WL-Test 判断为不同构，
    $\mathcal{A}$ 使用单射 $\phi$ 和 $f$ 更新嵌入向量：
    $$ h_v^{(k)} = \phi(h_v^{(k-1)}, f(\{h_u^{(k-1)} : u \in \mathcal{N}(v)\})) $$
    WL-Test 使用单射哈希函数 $g$ 更新结点标签：
    $$ L_v^{(k)} = g(L_v^{(k-1)}, \{L_u^{(k-1)} : u \in \mathcal{N}(v)\}) $$
    下证：对于任意轮迭代 $k$，存在单射 $\varphi^{(k)}$，使得 $h_v^{(k)} = \varphi^{(k)}(L_v^{(k)})$。

    $k = 0$ 时，$h_v^{(0)} = L_v^{(0)} = x_v$，故取 $\varphi^{(0)}$ 为恒等映射即可。

  \end{block}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图神经网络与 WL-Test 的比较}
  \begin{block}{证明：单射图神经网络的表达能力与 WL-Test 等价}
    $k \ge 1$ 时，假设第 $k - 1$ 轮迭代时存在单射 $\varphi^{(k-1)}$，则
    \begin{align*}
      h_v^k &= \phi(h_v^{(k-1)}, f(\{h_u^{(k-1)} : u \in \mathcal{N}(v)\})) \\
            &= \phi(\varphi^{(k-1)}(L_v^{(k-1)}), f(\{\varphi^{(k-1)}(L_u^{(k-1)}) : u \in \mathcal{N}(v)\})) \\
            &= \psi(L_v^{(k-1)}, \{L_u^{(k-1)} : u \in \mathcal{N}(v)\}) \\
            &= \psi \circ g^{-1} g (L_v^{(k-1)}, \{L_u^{(k-1)} : u \in \mathcal{N}(v)\}) \\
            &= \psi \circ g^{-1}(L_v^{(k)})
    \end{align*}
    其中，$\psi = \phi \circ \varphi^{(k-1)}$ 为单射。取 $\varphi^{(k)} = \psi \circ g^{-1}$ 即证。
    
    在最后一轮迭代时，WL-Test 得到的 $G_1$ 和 $G_2$ 结点标签集合不相同，说明 $\mathcal{A}$ 得到的嵌入向量集合不相同。
    
    又由于 $READOUT$ 是单射， $G_1$ 和 $G_2$ 的图嵌入向量不相同。
  \end{block}

\end{frame}
%---------------------------------------------------------

\section{图同构神经网络}

%---------------------------------------------------------
\begin{frame}

  \frametitle{图嵌入向量的计算}
  为使图神经网络具有和 WL-Test 相同的表达能力，需要选取恰当的映射用于图嵌入向量的计算。
  该映射需要满足以下条件：
  \begin{enumerate}
    \item 当自变量是集合时是良定的，即计算结果与结点顺序无关。 
    \item 是单射的，即不能把两个不同的向量集合映射到同一个向量。 
    \item 是可学习的，即该映射可以表示任意单射。
  \end{enumerate}
  $MEAN$ 和 $MAX$ 对集合是良定的，但不是单射。
  \ctikzfig{figs/mean-max}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{构造单射}
  \begin{alertblock}{定理：使用求和算子能构造可数集上的单射}
    (1) 设特征集合 $\mathcal{X}$ 是可数集，则存在函数 $f: \mathcal{X} \rightarrow \mathbb{R}^n$，使得
    对于 $\mathcal{X}$ 的任意有限大小的子集 $X$，$h(X) = \sum_{x \in X} f(x)$ 是单射。

    (2) 对于任意单射 $g(X)$，都存在单射$\phi$，使得 $g(X) = \phi(\sum_{x \in X} f(x))$。
  \end{alertblock}

  \begin{block}{证明：使用求和算子能构造可数集上的单射}
    (1) 由于 $\mathcal{X}$ 可数，故存在映射 $Z : \mathcal{X} \rightarrow \mathbb{N}$。
    又由于所有子集 $X$ 都是有限集，故存在 $N \in \mathbb{N}$，使得 $\forall X \subseteq \mathcal{X}, |X| < N$。
    取 $f(x) = N^{-Z(x)}$，则 $f(x)$ 相当于 $N$ 进制下仅小数点后 $Z(x)$ 位为 $1$ 的小数。$h(X) = \sum_{x \in X} f(x)$
    在集合意义下为单射。
    
    (2) 取 $\phi = g \circ h^{-1}$ 即可。
  \end{block}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图同构神经网络结构}
  \begin{alertblock}{推论：为中心结点的嵌入添加可学习的权重}
    (1) 设特征集合 $\mathcal{X}$ 是可数集，则存在函数 $f: \mathcal{X} \rightarrow \mathbb{R}^n$，使得
    对于 $\mathcal{X}$ 的任意元素 $c$ 和有限大小的子集 $X$，$h(X) = (1 + \epsilon) \cdot f(c) + \sum_{x \in X} f(x)$ 是单射。
    
    (2) 对于任意单射 $g(\mathcal{N}(v))$，都存在单射$\varphi$，使得 $g(\mathcal{N}(v)) = \varphi((1 + \epsilon) \cdot f(h_v) + \sum_{u \in \mathcal{N}(v)} f(h_u))$.
  \end{alertblock}
  结点嵌入的更新公式可以写为：
  \begin{align*}
    h_v^{(1)} &= \varphi^{(1)}((1 + \epsilon^{(1)}) \cdot f^{(1)}(x_v) + \sum_{u \in \mathcal{N}(v)} f^{(1)}(x_u)) \\
    h_v^{(k)} &= \varphi^{(k)}((1 + \epsilon^{(k)}) \cdot f^{(k)}(h^{(k-1)}_v) + \sum_{u \in \mathcal{N}(v)} f^{(k)}(h^{(k-1)}_u)), k > 1.
  \end{align*}
  接下来只需学习任意的 $\varphi$ 和 $f$ 即可。

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{图同构神经网络结构}
  结点嵌入的更新公式可以写为：
  \begin{align*}
    h_v^{(1)} &= \varphi^{(1)}((1 + \epsilon^{(1)}) \cdot f^{(1)}(x_v) + \sum_{u \in \mathcal{N}(v)} f^{(1)}(x_u)) \\
    h_v^{(k)} &= \varphi^{(k)}((1 + \epsilon^{(k)}) \cdot f^{(k)}(h^{(k-1)}_v) + \sum_{u \in \mathcal{N}(v)} f^{(k)}(h^{(k-1)}_u)), k > 1.
  \end{align*}
  $k = 1$ 时，由于 $h^0 = x$ 为结点的特征向量，如果取特征向量为独热码，则 $f^{(1)}$ 为恒等映射即满足单射性。

  $k > 1$ 时，将 $f^{(k)} \circ \varphi^{(k-1)}$ 看成一个整体，使用多层感知机拟合任意单射，得到图同构神经网络的更新公式：
  $$
    h_v^{(k)} = MLP^{(k)} ((1 + \epsilon^{(k)}) \cdot h_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} h^{(k-1)}_u))
  $$
  当迭代进行到最后一轮 $k = K$ 时，$MLP^{(K)}$ 已经将 $f^{(K)}$ 映射作用在了各个结点上，所以 $READOUT$ 函数只需将所有结点的嵌入相加即可生成图嵌入。
  在论文中，作者将每一轮迭代的图嵌入拼接起来作为最终的图嵌入。
  

\end{frame}
%---------------------------------------------------------

\section{分析与比较}

%---------------------------------------------------------
\begin{frame}

  \frametitle{非单射算子在某些情况下也可能取得很好的效果}
  \includegraphics[scale=0.3]{figs/other.png}
  \begin{enumerate}
    \item $MEAN$：适用于结点特征丰富，重复率低的任务，如文章主题分类和社区检测。
    \item $MAX$：适用于不关心结点特征的精确结构和分布，只需要识别代表元素或者骨架的任务，如图像分类。
  \end{enumerate}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{性能比较}
  \includegraphics[scale=0.25]{figs/test.png}

\end{frame}
%---------------------------------------------------------

\section{扩展工作}

%---------------------------------------------------------
\begin{frame}

  \frametitle{AAAI-19 Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks}
  \begin{block}{k-WL Test}
    将图 $G$ 中任意 $k$ 个相邻结点组成的元组 $V(G)^k$ 作为图中的原子对象。定义 $k$ 个结点组成的元组 $s$ 对应的第 $j$ 个邻域为：
    $$
      N_j(s) = \{(s_1, ..., s_{j-1}, r, s_{j+1}, ...,s_k) : r \in V(G)\}
    $$
    即将第 $j$ 个结点换成其他结点。

    由此定义可以将 WL-Test 扩展到 k-WL Test。首先对每组 $k$ 个结点构成的子图做 1-WL，再给被判断同构的子图赋予相同的初始标签。
    随后使用如下公式迭代更新第 $t$ 轮的标签值直到收敛：
    \begin{align*}
      C_j^{(t)}(s) &= HASH(\{c_{k}^{(t-1)}(s') : s' \in N_j(s)\}) \\
      c_k^{(t)}(s) &= HASH((c_k^{(t-1)}(s), (C_1^{(t)}(s), ..., C_k^{(t)}(s))))
    \end{align*}
    k-WL 利用了多个结点组成的子图的信息，表达能力严格强于 1-WL。
  \end{block}
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{AAAI-19 Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks}
  \includegraphics[scale=0.5]{figs/k-GNN.png}
  \begin{enumerate}
    \item 类比 k-WL，作者利用子图间的邻接关系将 GNN 拓展到 k-GNN，称之为高阶 GNN。
    \item k-WL 的表达能力强于 1-WL，故高阶 GNN 的表达能力也更强了。
  \end{enumerate}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{ICML-21 Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks}
  \begin{center}
    \includegraphics[scale=0.5]{figs/SWL.png}    
  \end{center}
  \begin{enumerate}
    \item 二维平面上的图结构难以捕捉高阶信息，故作者将图输入转换成单纯复形\footnote{由点、线段、三角形等单纯形粘合得到的拓扑对象。}（simplicial complexes），再进行消息传递。
    \item 在单纯复形的意义下进行同构测试被称作 SWL，SWL 的表达能力严格强于 WL，且不弱于 3-WL。
  \end{enumerate}

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}

  \frametitle{NeurIPS-21 Weisfeiler and Lehman Go Cellular: CW Networks}
  \begin{center}
    \includegraphics[scale=0.6]{figs/CWL.png}    
  \end{center}
  \begin{enumerate}
    \item 图结构中含有完全子图才能被转换成单纯复形，对图结构的要求过于严格。
    \item 本文定义了一种对图结构的提升变换，将对图结构的要求放宽到正则胞复形\footnote{由凸多面体粘合得到的拓扑对象。}（regular cell complexes）。本文提出的模型是目前图归约任务的最好结果。
  \end{enumerate}

\end{frame}
%---------------------------------------------------------



\end{document}